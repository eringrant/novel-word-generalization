\documentclass{tufte-book}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{lscape}
\usepackage{selfdefcommands}

\newcommand{\ind}{\hspace{1cm}}

\begin{document}

\chapter{Word learning}

\section{Nematzadeh, Fazly \& Stevenson: A cognitive model of semantic network learning}

\begin{easylist}[itemize]
    & model semantic knowledge as graph, where nodes = words / concepts and edges = semantic relations
        && Steyvers and Tenenbaum showed that modelled semantic network has small-world, scale-free structure (sparsity)
            &&& TODO: examine their model
    & constraints on the model to enforce cognitive plausibility:
        && incrementality
        && limited computations at each step
    & cognitive models of categorisation:
        && Sanborn (2010)
        &&
    & model mapping of words to word meanings as a cross-situational learning problem; grow a semantic networks using the incremental knowledge
\end{easylist}

\newthought{Cross-situational model}

\begin{easylist}[itemize]
    & meaning of a word is a probability distribution over features $P(.\mid w)$.
    & learning is equivalent to inferring the value of the latent variables $a_{ij}$ for each mapping (word, feature) in the scene:
    
    \begin{align*}
        P(a_{ij} \mid u, f_i) = \frac{P_{t-1}(f_i \mid w_j)}{\sum_{w'\in u} P_{t-1} (f_i | w')} \tag{learn.py, line 411}\\
    \end{align*}

        && if $w$ not in lexicon (not yet heard), $P_{t-1}(f_i \mid w_j) = 0$ \hfill (wmapping.py, line 137)
        && if $w$ in lexicon but feature not yet associated, $P_{t-1}(f_i \mid w_j) \leftarrow \tfrac{\lambda}{\text{sum assoc}}$ \hfill (wmapping.py, line 136)
            &&& this is the probability of an unseen feature given a word (uniform prior with $\lambda$ term)
            &&& $\lambda :=$ either manually set parameter or $\frac{1}{1 + \text{time} \times \text{power}}$, where power is a param \hfill (learn.py, line 231)
            &&& sum assoc $:=$ the sum of $P_{t-1}(f_i \mid w_j)$ for all \emph{seen} features $f_i$
                &&&& $\lambda$, sum assoc updated when meaning probability for any word is updated \hfill (learn.py, 295-313)

    & learned meanings are then updated:

    \begin{align*}
        P_t(f_i \mid w_j) = \frac{\sum_{u \in U_t} P (a_{ij} \mid u, f_i)}{\sum_{f'\in M}\sum_{u \in U_t} P (a_{ij} \mid u, f_i)}
    \end{align*}

        && $U_t$ is set of heard utterances
        && $M$ is set of observed features
        && $P_t(f_i \mid w_j)$ increases additively (but with normalisation), since we sum $P (a_{ij})$ over ${u \in U_t}$
\end{easylist}

\newthought{Semantic network model}

\begin{easylist}[itemize]
    & learn words incrementally as above
    & requirements:
        && change existing connections as needed (just removal?)
        && create new connections as needed
        && both must be done in a cognitively plausible manner; i.e., avoiding a large number of computations at any step
            &&& obvs all existing connections to a word must be checked for removal
            &&& but cannot check every pairwise connection with all nodes in the network $\implies$ must restrict to a subset of edges $S$

    & how to choose $S$
\end{easylist}


\newpage

\section{Abbot, Austerweil \& Griffiths: Constructing a hypothesis space from the Web for large-scale Bayesian word learning}

\newthought{Problems with previous models of word learning}

\begin{easylist}[itemize]
    
    & e.g., Xu and Tenenbaum 2007:
        && hypothesis space and prior are hand-coded $\implies$ possibly the results are due to a modelling decision
    & proposed solution: using large databases of public information (WordNet and ImageNet) to automatically construct hypothesis space and priors
        && since ImageNet is organised to reflect the hierarchy of WordNet, can use the images for accumulating behavioural data
\end{easylist}

\newthought{Formulation of the generalisation problem}

\begin{easylist}[itemize]

    & use Bayes' Rule to model probability of generalisation
        && observe $n$ positive examples $\x= (x_1, \hdots, x_n)$ of a concept / class $\mathcal{C}$; want to calculate $p(y \in \mathcal{C} \mid \x)$
        && use hypothesis space $\mathcal{H}$, a set of concepts, each of which has a corresponding $P(\mathbf{x} \mid h)$
        && the work of the model is in specifying the hypothesis space $\mathcal{H}$, a prior over hypotheses, and a likelihood $P(\x \mid h)$ for each hypothesis $h$
            &&& likelihood:
                &&&& use typical likelihood in generalisation framework: strong sampling (objects generated uniformly at random from the true hypothesis (Tenenbaum \& Griffiths, 2001))
                &&&& from Xu and Tenenbaum (2007):

                $$P(\x \mid h) = 1/|h|^n \text{ if } \x \subset h$$

                or zero otherwise
                &&&& therefore hypothesis with fewer objects are considered more likely
                &&&& \emph{motivation for the size principle is dependent on assumption of uniform distribution of examples}
            &&& prior can be:
                &&&& uniform over hypothesis space (Shepard, 1987) $\leftarrow$ this is what Xu and Tenenbaum use
                &&&& stochastic over tree structures (Kemp \& Tenenbaum, 2009)
            &&& then, the probability that a new example belongs to a certain concept is 
                
                $$p(y \in \mathcal{C} \mid \x) = \sum_{h\in\mathcal{H}} P(y\in\mathcal{C}\mid h)P(h\mid\x)$$

                where $p(y \in \mathcal{C} \mid \x) = 1$ if the new object belongs to hypothesis $h$ (i.e., if the hypothesis says that $y$ belongs to the concept $\mathcal{C}$), and 0 otherwise

\end{easylist}

\newthought{Word learning in Xu and Tenenbaum (2007)}

\begin{easylist}[itemize]
        && hierarchical clustering of behavioural similarity data to generate tree-structured hypothesis space
        && same tree then used to define prior and likelihood
            &&& motivation for tree structure: ``children assume possible referents of novel nouns are tree-structured'' (Markham, 1991)
        && nodes are concepts = words
            &&& objects falling under a given concept are the children of the node
            &&& height of a node $\propto$ average pairwise dissimilarity of objects covered by node $\approx$ heterogeneity of the concept
                &&&& $\iff$ the greater the height of a node, the more distinctive the items within the cluster
        && then define the prior as follows:

        $$P(h) \propto \text{height}(\text{parent}(h)) - \text{height}(h)$$

            &&& also directly incorporated a basic-level bias by increasing the prior of the basic levels hypotheses by a factor of 10
        && likelihood defined as follows, for a set of examples $\x$:

        $$P(\x \mid h) \propto \left[\frac{1}{\text{height}(h) + \epsilon}\right]^n$$
            &&& $\epsilon :=$ small constant so that the leaf hypotheses do not have infinite likelihood
            &&& this likelihood function is an implementation of the size principle, since the height of the node $\approx$ number of objects in extension; \ie clusters with greater dissimilarity are presumed to have more instances

        && results:
            &&& could predict how people extend words to novel concepts, dependent on diversity and cardinality of set of examples
                &&&& \eg given one example: select subordinate or basic level match
                &&&& \eg given at least three examples: select only subordinate level match
            &&& predicted by the model, since prior favours basic level while likelihood favours subordinate level, and likelihood's weight increases with number of examples
\end{easylist}

\newthought{Bayesian generalisation framework using WordNet}

\begin{easylist}[itemize]
& hypotheses are subsets of the universe of objects to which a given concept applies
    && with WordNet: universe of objects := leaf nodes (instances / examples)
    && hypotheses := non-leaf nodes, as well as leaf nodes, allowing for discrimination between concepts at the level of observed examples
    && construction of the WordNet tree:
        &&& extract a directed acyclic graph from noun nodes of WordNet; $a$ has a directed edge to $b$ if $b$ is an instance of $a$
        &&& nodes are hypotheses = words / concepts
        &&& hypothesis space := binary matrix $\mathcal{H}$, where rows are examples (leaf nodes) and columns correspond to hypotheses
            &&&& if $\mathcal{H}[i,j] = 1$ then $j$ is an ancestor of leaf node $i$ in the WordNet graph
\end{easylist}

\newthought{Model implementation}

\begin{easylist}[itemize]
    & given a set of examples $\x = \{x_1, \cdots, x_n\}$ representing some concept $\mathcal{C}$ (examples are the rows of the matrix $\mathcal{H}$), compute $$P(y \in \mathcal{C} \mid \x)$$
    & tested three methods of generalisation:
        && Bayesian method:
        $$\sum_{h\in\mathcal{H}}P(y\in\mathcal{C}\mid h)P(h\mid\x) = \sum_{h\in\mathcal{H}}P(y\in\mathcal{C}\mid h)P(\x\mid h)P(h)$$
        where the prior distribution over hypotheses is Erlang distributed (standard for prior over sizes) and given by
        $$P(h) \propto (|h|/\sigma^2)\exp\{-|h|/\sigma\}$$
            &&& $\sigma$ parameter hand-fit to human responses
        && `Prototype method':
        $$\exp\{-\lambda_p\cdot\text{Hamming distance}(y, x_\text{proto})\}$$
            &&& where $x_\text{proto}$ is a (constructed?) example with the majority features shared by a concept
            &&& score then normalised over examples (leaf nodes)
        && `Exemplar method':
        $$\sum_{x_j \in \x} \exp\{-\lambda_e \cdot \text{Hamming distance}(y, x_j)\}$$
            &&& where $\lambda_e$ is a free parameter
            &&& $x_j\in\x \implies$ distance computed for each example
            &&& score then normalised over examples (leaf nodes)
\end{easylist}

\newthought{Experiments}

\begin{easylist}[itemize]
    & experiment 1: participants from Amazon Mechanical Turk and models on an identical set of data:
        && images from subordinate, basic and super-ordinate levels
        && training set: images + a word to reference the images
            &&& four conditions of images for the training set:
                &&&& one subordinate example (Dalmatian)
                &&&& three subordinate examples of the same subordinate type (three Dalmatians)
                &&&& the subordinate object and two basic-level objects (Dalmatian, shih tzu, beagle)
                &&&& the subordinate and two super-ordinate-level objects (Dalmatian, hippo, toucan)
        && test set the same for all conditions:
            &&& two subordinate examples, two basic-level examples, four super-ordinate examples, sixteen non-matching objects
        && objective: generalise the word encountered in the training set to a subset of entities in the test set
    & experiment 2: participants from Amazon Mechanical Turk and models on an identical set of data
        && incorporated more concepts and examples
\end{easylist}

\newthought{Experiments}

\begin{easylist}[itemize]
            & experiment 1:
                && human judgment and Bayesian models average judgment correlated most highly ($r^2 = 0.98$); prototype and exemplar less so
                && human and Bayesian pattern: 3 subordinate example condition = most likely to generalise to subordinate level, whereas in one subordinate example condition, moderate probability of generalisation to basic level
& experiment 2:
    && generalisation of humans and Bayesian model again correlated
    && Bayesian model also captured idiosyncrasy of some hypotheses: generalise more broadly in some domains than others
        &&& claim: function of the distinctiveness property encoded in the model
    && however, large amount of variance between human and model predictions
        &&& claim: some domains do not conform to a natural taxonomy, despite the fact that they are organised that way on WordNet
\end{easylist}

\newthought{Conclusions}

\begin{easylist}
    & Bayesian model predictions based on this WordNet hierarchy are highly correlated with human judgments $\implies$ using WordNet hierarchy is a valid method for generating generalisation judgments
    & future work:
        && larger-scale data
        && `more heterogeneous training sets' (e.g., one subordinate-level and one basic-level object)
        && interaction of visual similarity with conceptual structure
        && generalisation in other domains than word learning
\end{easylist}

\newpage


\newpage

\section{Jia, Abbot, Austerweil, Griffiths, \& Darrel: Visual concept learning: Combining machine vision and Bayesian generalisation on concept hierarchies}

\newthought{Motivation}

\begin{easylist}[itemize]
    & `state of the art machine vision systems' have trouble accounting for patterns of human generalisation
        && e.g., human generalisation patterns change as more instances of the same example are observed
        && e.g., without negative example, machine learning systems are unable to choose between valid hypotheses (if only observe Dalmatians, both `Dalmatian' and `dog' are valid labels)
    & note that models of Bayesian generalisation are unable to make generalisation predictions about truly novel stimuli, since the object must be already mapped into the hierarchical hypothesis tree
        && correct that in this model by modelling uncertainty about recognition of examples with an image classifier

\end{easylist}

\newthought{Generation of test data}

\begin{easylist}[itemize]
    & use ImageNet hierarchy to generate concepts in the following manner:
        && generate three levels of abstraction from the path from each leaf node to the root (if concepts overlap, prune the hierarchy)
            &&& node chosen to maximise information gain:

            $$\mathcal{C}(L_{1\hdots 3}) = \sum_{i=0}^3 \log(|L_{i+1}| = |L_i|) - \log|L_{i+1}|$$

                &&&& $L_i$ is the number of leaf nodes under the subtree rooted at $L_i$, and $L_4$ is the whole tree
        && across all levels, 4000 concepts generated
    & ground truth of human generalisation of concepts to images was obtained through Amazon Mechanical Turk

\end{easylist}

\newthought{Model description}

\begin{easylist}
    & hierarchy as defined above
        && each leaf node is what can be observed (the examples)
        && each (leaf and non-leaf) node corresponds to a possible hypothesis for a concept, where the children of a node also belong to that hypothesis
        &&& size of hypothesis is the cardinally of the tree rooted at the hypothesis
    & use Bayesian framework, incorporating uncertainty introduced by image classification with a confusion matrix:
        && likelihood:

        $$P_\text{example} (\x_i \mid h) = \sum_{j=1}^K A_{j\hat{y}_i} \frac{1}{|h|}I(j\in h)$$

            &&& if there are $K$ leaf nodes, and an image  $\x_i$ is classified as $\hat{y}_i$, then the likelihood of an example given a hypothesis is inversely proportional to the size of the hypothesis
            &&& $A$ is the confusion matrix, and $A_{j\hat{y}_i}$ is the probability that the true leaf node is $j$ given the classifier output being $i$
                &&&& marginalise over all the possible leaf nodes given the classifier output
                &&&& the confusion matrix generated by approximation of leave-one-out validation methods in the training of the image classifiers
        && generalisation function:
        (the likelihood without the size term)

        $$P_\text{new} (\x_\text{new} \mid h) = \sum_{j=1}^K A_{j\hat{y}_i} I(j\in h)$$


\end{easylist}

\newthought{Experiment}

\begin{easylist}
    & given five example images that belong to a concept, which of the twenty query images also belong to the concept represented by the examples?
    & four levels of query conditions:
        && $L_0$: items from the leaf-level (e.g., Dalmatian)
        && $L_1$: items from the basic level (e.g., different types of dogs)
        && $L_2$: items from the super-ordinate level (e.g., different types of animals)
        && $L_3$: items from the super-super-ordinate level (e.g., different types of living things)
    & baseline models:
        && naive vision approach: 
            &&& compute distance using GIST image features between each query image and the closest example, which belongs to the concept
            &&& if distance low, then query belongs to the concept
        && prototype model: 
            &&& use as a vector for the query image the $L_1$ normalised classifier output from multinomial logistic regressor
            &&& if $\chi^2$ distance to the closest example is low, then query belongs to the concept
        && histogram of classifier outputs: 
            &&& same as prototype model, but aggregate distance to the given examples instead of just using the closest example
        && hedging the bets extension:
            &&& modify the concept (hypothesis node in the hierarchy) to instead `hedge bets'
                &&&& find a subtree in the hierarchy that maximises information gain but has accuracy above a threshold $\epsilon$ over the set of examples; $\epsilon$ is a tuned parameter
            &&& compute the probability of the query belonging to this subtree (using a naive image classifier?)
        && non-perceptual word learning (as in Xu \& Tenenbaum (2007)):
            &&& assumes a perfect classifier
        \end{easylist}

\newthought{Results}

\begin{easylist}
    & simple vision models fail to come as close to human performance as the Bayesian generalisation model, since they do not consider number of instances seen
    & the non-visual word learning model performs better than the visual word-learning model
        && claim: classifiers are not good enough yet to emulate human performance in visual recognition $\implies$ bottleneck in the performance of their new model
    & all visual models have higher precision when recall is low
        && claim: perceptual signals play an important role in human generalisation behaviours; should be incorporated into future models
    & Bayesian model qualitatively matches human performance:
        && decrease in generalisation as the query level moves further from the example level (both directions: from abstract to specific and specific to abstract)
        && generalisation probability peaks when query examples are chosen from the same level as the examples
            &&& (but this is also shown in the vision baseline)

\end{easylist}

\newpage

\section{Xu \& Tenenbaum: Word learning as Bayesian inference}

\newthought{Model}



\setlength{\tabcolsep}{12pt}

\begin{tabular}{rl}
    $C$
    & the novel word
    \\\\
    $X = \{x^{(1)}, \hdots, x^{(n)}\} \subset U$
    & \specialcell{the set of observed examples of the target\\ word $C$, taken from the domain of entities $U$}
    \\\\
    $h_1, h_2, \hdots \in \mathcal{H}$
    & hypotheses about which subset of $U$ is referred to by $C$
    \\\\
    $p(X \mid h)$
    &  \specialcell{likelihood of the data given the hypothesis \\
                    \quad 
                    could capture: \\
                    \qquad
                    - assumption that the data is a representative sample of the concept\\
                    \qquad
                    - effect of syntactic context\\
                    \qquad
                    - contrast with other words in the environment\\
                }
    \\\\
    $p(h)$
    &  \specialcell{prior probability of the hypothesis \\
                    \quad 
                    could capture: \\
                    \qquad
                    - conceptual, lexical, contextual constraints\\
                    \qquad
                    - plausibility of a hypothesis conditioned on prior learned words\\
                    \qquad
                    - taxonomic constraints and basic-level bias\\
                }
    \\\\
    $y$ & the novel object\\\\
    $p(y\in C \mid X)$
    &  \specialcell{probability of generalisation given the observed examples\\
                    \quad 
                    - captures the probability that the test object falls under $C$\\
                }
                \\\\
    $p(y\in C \mid h)$
    &  \specialcell{probability of generalisation given a hypothesis of $C$'s meaning\\
                    \quad 
                    $= 1$ iff $y\in h$, and $0$ otherwise
                }

\end{tabular}

\vspace{1cm}

The learned meaning of a word is the posterior probability of each hypothesis:
\begin{align*}
    p(h \mid X) 
    &= \frac{p(X\mid h)p(h)}{p(X)}\\
    &= \frac{p(X\mid h)p(h)}{\sum_{h'\in H} p(X \mid h')p(h')}.
\end{align*}

Generalisation is tested by computing the following probability:
\begin{align*}
    p(y\in C \mid X) 
    &= \sum_{h\in H} p(y\in C \mid h)p(h\mid X)\\
    &= \sum_{h\supset y, X} p(h\mid X).
\end{align*}

\section{Analogy to our model}

\begin{tabular}{rl}
    $w$
    & the word being tested
    \\\\
    $P(\cdot \mid w)$
    & \specialcell{the observed data, in the form of a \\ probability distribution over features given $w$}
    \\\\
\end{tabular}

Here, Xu and Tenenbaum have something that is not readily analogical to our model: a set of hypotheses $\mathcal{H}$ that describe whether the novel word $C$ should be extended to objects with which it has not yet been associated.

In our model, this would be equivalent to the following: Given a novel object, (i.e., some object of which a subset of features is novel), the model must infer, where $\mathcal{F}$ is the set of observed features:

\begin{center}{$p(\mathcal{F} \mid w).$}\marginnote{$p(\text{test scene} \mid \text{fep, the learned lexicon})$}\end{center}

If this computation exceeds a threshold, then the novel object falls under the word-concept $w$.
\marginnote{We could also marginalise over all the probable meanings of ``fep'':

    \medskip
    $p(\text{test scene} \mid \text{fep}) \cdot p(\text{fep} \mid \text{lexicon})$
}

If we assume that all word-concepts are equiprobable, then we can model categorisation of a given object as finding the word which maximises the following computation:
\begin{align*}
    p(w\mid \mathcal{F})
    &= \frac{p(\mathcal{F} \mid w)}{\sum_{w'\in\mathcal{W}}p(\mathcal{F} \mid w)}.
\end{align*}

This is the Shepard-Luce choice rule: given the probability of classifying $X$ as a $C$ is given by $X$'s similarity to $C$, divided by the sum of $X$'s similarity to every possible category, including $C$.

If we do not make the assumption that each word-concept is equiprobable, then we must incorporate some prior information about the probability of the word:\marginnote{e.g., corpus frequency}
\begin{align*}
    p(w\mid \mathcal{F})
    &= \frac{p(\mathcal{F} \mid w)p(w)}{\sum_{w'\in\mathcal{W}}p(\mathcal{F} \mid w')p(w')}.
\end{align*}

\newthought{How is $p(\mathcal{F} | w)$ computed?}

\marginnote{
Possible sources of information, if the learner has observed the feature:

    - frequency of the feature in the observed data

    - constraints on the unconditional probability of a feature; e.g., context-independent salience

    - if the learner's lexicon has a categorical / taxonomic structure, the learner could exploit the categorical structure to infer the likelihood of a feature $f$ given the other features in the scene (i.e., the learner has knowledge about which features are commonly associated with each other)

}
For features as yet unobserved with the word, this is equivalent to feature inference: given a word, inference of the probability of unassociated features. 

If the learner has never observed the feature, then $p(f | w)$ should be uniform over words.

If the learner has observed the feature (with other words), the model could exploit this knowledge to infer $p(f | w)$.

\newpage

\begin{landscape}
\section{Dan's model}

%\begin{landscape}

\begin{align*}
    \begin{array}{c}\text{generalisation} \\ \text{probability}\end{array}
    &=\sum_{w \in\, \text{lexicon}}
    \left[
    \left(
    \frac{\begin{array}{c}\text{cosine} \\ \text{similarity}\end{array}\text{($w$, ``fep'')}}
    {\sum_{w' \in\, \text{lexicon}}\begin{array}{c}\text{cosine} \\ \text{similarity}\end{array}\text{($w'$, ``fep'')}}
    \right)
    \left(
    \frac{\begin{array}{c}\text{cosine} \\ \text{similarity}\end{array}\text{($w$, test item)}}
    {\sum_{w' \in\, \text{lexicon}}\begin{array}{c}\text{cosine} \\ \text{similarity}\end{array}\text{($w'$, test item)}}
    \right)
    \left(
    \frac{\text{frequency($w$)}}
    {\sum_{w' \in\, \text{lexicon}}\text{frequency($w'$)}}
    \right)
\right]
\end{align*}

%\end{landscape}

\section{Endress}
\begin{align*}
    \left(\frac{\text{similarity of test object $t$ to observed ``fep''}}
    {\sum_{t' \in\, \text{test scene}}\text{similarity of test object $t'$ to observed ``feps''}}\right)^\text{number of examples of ``fep'' observed}
\end{align*}

\begin{align*}
    \left(\frac{\begin{array}{c}\text{similarity of test object $t$}\\\text{to observed ``fep$_1$''}\end{array}}
    {\sum_{t' \in\, \text{test scene}}\begin{array}{c}\text{similarity of test object $t'$}\\\text{to observed ``fep$_1$''}\end{array}}\right)
    \times
    \cdots
    \times
    \left(\frac{\begin{array}{c}\text{similarity of test object $t$}\\\text{to observed ``fep$_n$''}\end{array}}
    {\sum_{t' \in\, \text{test scene}}\begin{array}{c}\text{similarity of test object $t'$}\\\text{to observed ``fep$_n$''}\end{array}}\right)
\end{align*}

\end{landscape}
\end{document}
