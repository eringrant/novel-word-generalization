# This is the config file for the Xu & Tenenbaum generalisation probability experiments.

[DEFAULT]
path = ./results_to_discard
repetitions = 1
iterations = 1

lexname = norm_prob_lexicon_cs.all
#corpus-path = ['input_wn_fu_cs_scaled_categ.dev']
corpus-path = ['generate']
hierarchy = ['xt_hierarchy_one_feature.xml']

maxtime = 100000000 # the number of sentences the learner is exposed to prior

# config file parameters
dummy =                             [False]
forget =                            [False]
novelty =                           [False]

power =                             [1]
#power = 1 is ND; power = 0.5 is LT_.5 (late talker less severe),; power = 0.25 is LT_.25 (late talker more severe)

# how many of each word in the pretraining corpus
num-subordinate =                   [100, 500, 1000]
num-basic-level =                   [300, 1500, 3000]
num-superordinate =                 [900, 4500, 9000]

# generation of features in the pretraining corpus
prob =                              [True, False]

# weighting of features in the test scene
basic-level-bias =                  [0.5, 2, 3, 4, None] # relative weight factor

# ordered list of parameters with which to name and annotate the graph
graph-annotation =                  [['beta', 'lambda', 'prob', 'num-super', 'num-basic', 'num-sub', 'basic-level-bias', 'calculation-type']]

calculation-type =                  ['cosine', 'cosine-norm']
std =                               None

[Beta 5000]

beta =                              [5000]
lambda =                            [1.0/5000]

[Beta 10000]

beta =                              [10000]
lambda =                            [1.0/10000]

#[Gaussian experiments]
#
#calculation-type =                  ['gaussian']
#std =                               [0.001, 0.005, 0.01, 0.05]
